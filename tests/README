# ðŸ§ª Sistema de Testing del Trading Bot

GuÃ­a completa para ejecutar y crear tests de integraciÃ³n para el trading bot.

---

## ðŸ“‹ Tabla de Contenidos

1. [InstalaciÃ³n](#instalaciÃ³n)
2. [Ejecutar Tests](#ejecutar-tests)
3. [Tipos de Tests](#tipos-de-tests)
4. [Cobertura de Tests](#cobertura-de-tests)
5. [Escribir Nuevos Tests](#escribir-nuevos-tests)
6. [CI/CD](#cicd)

---

## ðŸš€ InstalaciÃ³n

### Prerrequisitos

```bash
# Python 3.9+
python --version

# PostgreSQL corriendo (para tests de integraciÃ³n)
docker-compose up -d postgres
```

### Instalar Dependencias

```bash
# Instalar todas las dependencias (incluyendo testing)
pip install -r requirements.txt

# O instalar solo las de testing
pip install pytest pytest-flask pytest-cov pytest-mock
```

### Verificar InstalaciÃ³n

```bash
pytest --version
# pytest 7.4.3
```

---

## â–¶ï¸ Ejecutar Tests

### MÃ©todos Disponibles

#### **1. Script Python (Recomendado)**

El script `run_tests.py` facilita ejecutar diferentes tipos de tests:

```bash
# Todos los tests
python run_tests.py

# Solo tests rÃ¡pidos
python run_tests.py fast

# Solo tests de integraciÃ³n
python run_tests.py integration

# Solo tests del dashboard
python run_tests.py dashboard

# Con coverage
python run_tests.py --cov

# Con reporte HTML
python run_tests.py --cov --html

# Modo verbose
python run_tests.py -v

# Ver prints de los tests
python run_tests.py -s

# Tests especÃ­ficos
python run_tests.py -k "test_api_account"

# Solo tests que fallaron antes
python run_tests.py --failed
```

#### **2. Pytest Directamente**

```bash
# Todos los tests
pytest tests/

# Tests especÃ­ficos
pytest tests/test_dashboard_integration.py

# Una funciÃ³n especÃ­fica
pytest tests/test_dashboard_integration.py::test_api_account

# Con verbose
pytest tests/ -v

# Con prints
pytest tests/ -s

# Por markers
pytest -m integration  # Solo integraciÃ³n
pytest -m "not slow"   # Excluir lentos
```

#### **3. Tests por CategorÃ­a**

```bash
# Tests unitarios (rÃ¡pidos)
pytest -m unit

# Tests de integraciÃ³n (requieren BD)
pytest -m integration

# Tests del dashboard
pytest -m dashboard

# Tests de API
pytest -m api

# Solo tests rÃ¡pidos
pytest -m "not slow"

# Solo tests lentos
pytest -m slow
```

---

## ðŸ·ï¸ Tipos de Tests

### **Dashboard Integration Tests** (`test_dashboard_integration.py`)

Prueba todos los endpoints del dashboard:

| Test | DescripciÃ³n | Requiere |
|------|-------------|----------|
| `test_index_page` | PÃ¡gina principal carga | - |
| `test_api_account` | Info de cuenta | API |
| `test_api_positions` | Posiciones abiertas | API |
| `test_api_config` | ConfiguraciÃ³n | - |
| `test_api_status` | Estado del bot | - |
| `test_api_trades_history` | Historial de trades | BD |
| `test_api_trades_stats` | EstadÃ­sticas | BD |
| `test_api_export_trades_csv` | Export CSV | BD |
| `test_api_export_trades_excel` | Export Excel | BD |
| `test_api_export_full_report` | Reporte completo | BD |
| `test_api_sessions_list` | Listar sesiones | BD |
| `test_api_session_detail` | Detalle sesiÃ³n | BD |
| `test_api_backtest_run` | Ejecutar backtest | API |
| `test_api_signals_recent` | SeÃ±ales recientes | BD |
| `test_api_health` | Health check | - |

### **Markers Disponibles**

```python
@pytest.mark.unit          # Tests unitarios rÃ¡pidos
@pytest.mark.integration   # Tests de integraciÃ³n
@pytest.mark.dashboard     # Tests del dashboard
@pytest.mark.api          # Tests que requieren API
@pytest.mark.slow         # Tests lentos (>5s)
```

---

## ðŸ“Š Cobertura de Tests

### Generar Reporte de Coverage

```bash
# Ejecutar con coverage
pytest --cov=. --cov-report=html tests/

# O usar el script
python run_tests.py --cov --html
```

### Ver Reporte HTML

```bash
# Windows
start htmlcov/index.html

# Mac/Linux
open htmlcov/index.html

# O navegar manualmente a:
# file:///path/to/project/htmlcov/index.html
```

### MÃ©tricas de Coverage

El reporte muestra:
- **LÃ­neas totales** en cada archivo
- **LÃ­neas ejecutadas** durante los tests
- **LÃ­neas no cubiertas** (falta testear)
- **% Coverage** por archivo

**Meta: >80% coverage en mÃ³dulos crÃ­ticos**

---

## âœï¸ Escribir Nuevos Tests

### Estructura de un Test

```python
import pytest

def test_nombre_descriptivo(client, db_manager):
    """
    DescripciÃ³n clara de quÃ© se estÃ¡ probando
    """
    # Arrange - Preparar datos
    session_id = db_manager.start_session(10000.0, {})
    
    # Act - Ejecutar acciÃ³n
    response = client.get('/api/endpoint')
    data = response.json()
    
    # Assert - Verificar resultado
    assert response.status_code == 200
    assert 'key' in data
    assert data['value'] > 0
```

### Usar Fixtures

```python
def test_with_fixtures(test_session, test_trades):
    """
    Fixtures disponibles:
    - client: Cliente Flask
    - db_manager: DatabaseManager
    - test_session: SesiÃ³n de prueba creada
    - test_trades: Trades de prueba
    - test_signals: SeÃ±ales de prueba
    - mock_api_client: Mock de API
    """
    # Tu cÃ³digo aquÃ­
    pass
```

### Ejemplo Completo

```python
import pytest

@pytest.mark.integration
@pytest.mark.dashboard
def test_export_trades_with_session(client, test_session, test_trades):
    """
    Test: Exportar trades de una sesiÃ³n especÃ­fica a CSV
    
    Given: Una sesiÃ³n con 5 trades
    When: Se solicita exportar a CSV
    Then: Se descarga un archivo CSV vÃ¡lido
    """
    # Act
    response = client.get(f'/api/trades/export/csv?session_id={test_session}')
    
    # Assert
    assert response.status_code == 200
    assert 'text/csv' in response.content_type or 'application/octet-stream' in response.content_type
    
    # Verificar que el contenido tiene datos
    content = response.data.decode('utf-8')
    assert 'epic' in content.lower()
    assert 'pnl' in content.lower()
```

### Tests Parametrizados

```python
@pytest.mark.parametrize('endpoint,expected_keys', [
    ('/api/account', ['balance', 'available']),
    ('/api/config', ['assets', 'max_positions']),
    ('/api/status', ['status', 'is_trading_hours'])
])
def test_endpoints_have_required_keys(client, endpoint, expected_keys):
    """Test que mÃºltiples endpoints devuelven las keys correctas"""
    response = client.get(endpoint)
    
    if response.status_code == 200:
        data = response.json()
        for key in expected_keys:
            assert key in data
```

---

## ðŸŽ¯ Mejores PrÃ¡cticas

### âœ… DO (Hacer)

- **Nombres descriptivos**: `test_export_trades_returns_csv_file`
- **Tests independientes**: Cada test debe poder ejecutarse solo
- **Usar fixtures**: Reutilizar configuraciÃ³n comÃºn
- **Limpiar despuÃ©s**: Usar `yield` en fixtures
- **Documentar**: Docstring explicando el propÃ³sito
- **Asserts claros**: Mensajes descriptivos
- **Fast first**: Tests unitarios rÃ¡pidos primero

### âŒ DON'T (No hacer)

- Tests dependientes entre sÃ­
- Hardcodear valores (usar fixtures/config)
- Tests sin asserts
- Ignorar tests que fallan
- Tests muy complejos (dividir en varios)
- Modificar BD de producciÃ³n

---

## ðŸ”§ Troubleshooting

### Problema: Tests fallan por BD

```bash
# Verificar que PostgreSQL estÃ¡ corriendo
docker ps | grep postgres

# Iniciar PostgreSQL
docker-compose up -d postgres

# Verificar conexiÃ³n
python -c "from database.connection import DatabaseConnection; db = DatabaseConnection(); print('âœ… Connected')"
```

### Problema: Tests fallan por API

Los tests de API pueden fallar si:
- No hay conexiÃ³n a internet
- Credenciales invÃ¡lidas
- API de Capital.com caÃ­da

**SoluciÃ³n:** Usar `mock_api_client` fixture

```python
def test_with_mock_api(mock_api_client):
    """Test sin conexiÃ³n real a API"""
    account = mock_api_client.get_account_info()
    assert account['balance']['balance'] == 10000.0
```

### Problema: Imports no funcionan

```bash
# Verificar que el proyecto estÃ¡ en PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:$(pwd)"

# O ejecutar desde raÃ­z del proyecto
cd /path/to/trading_bot
pytest tests/
```

---

## ðŸš¦ CI/CD

### GitHub Actions Example

```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
      
      - name: Run tests
        run: |
          python run_tests.py --cov
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3
```

---

## ðŸ“š Recursos

- [Pytest Documentation](https://docs.pytest.org/)
- [Flask Testing](https://flask.palletsprojects.com/en/3.0.x/testing/)
- [Coverage.py](https://coverage.readthedocs.io/)

---

## ðŸ†˜ Ayuda

Si tienes problemas con los tests:

1. Verifica que todas las dependencias estÃ©n instaladas
2. AsegÃºrate de que PostgreSQL estÃ¡ corriendo
3. Ejecuta tests en modo verbose: `pytest -vv`
4. Revisa los logs: `pytest -s`
5. Ejecuta un test individual para debugging

---

**Â¿Preguntas?** Revisa la documentaciÃ³n o abre un issue en el proyecto.